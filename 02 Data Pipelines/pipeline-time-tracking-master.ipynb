{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This project serves as a pre-processing step for other portfolio projects with the goal of using a subset of data from my business, Tryba Music LLC, in a public format while preserving financial and client privacy. Data is pulled from my Google Sheets time tracking and project management databases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from Google Sheets\n",
    "def google_sheets_to_dataframes(sheet_list):\n",
    "    \n",
    "    dfs = {}\n",
    "\n",
    "    for sheet_name in sheet_list:\n",
    "        \n",
    "        # Define the URL for the API call\n",
    "        url = \"google_sheet_url\" + sheet_name\n",
    "        \n",
    "        df = pd.read_csv(url)\n",
    "\n",
    "        # Strip extra spaces in column names before writing .csv\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # add dataframe to dictionary\n",
    "        dfs[sheet_name] = df\n",
    "\n",
    "    return dfs\n",
    "\n",
    "# list of sheets to make dataframes\n",
    "sheet_names = ['time_tracking', 'project_hours']\n",
    "\n",
    "# get dataframes\n",
    "dfs = google_sheets_to_dataframes(sheet_names)\n",
    "\n",
    "# assigns the DataFrame (value) to a global variable with a name equal to the current sheet name\n",
    "for key, value in dfs.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "# Clarify names of dataframes for vs code interpreter formatting (makes it look better)   \n",
    "time_tracking = time_tracking\n",
    "project_hours = project_hours\n",
    "\n",
    "print(\"Data loaded to dataframes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Hours Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only projects that are done and projects that are time tracked\n",
    "project_hours = project_hours[(project_hours['time_tracked'] == True) & (project_hours['status'].str.lower() == \"done\")]\n",
    "\n",
    "# filter to only desired columns\n",
    "columns_to_drop = ['first_installment_paid', 'second_installment_paid', 'project_hourly', 'percent_budget_used',\n",
    "                   'amount_paid', 'amount_pending', 'days_since_funded', 'days_to_start', 'days_to_finish', \n",
    "                   'days_between_payments', 'cataloged', 'licensing_agent', 'collab_agreement_signed',\n",
    "                   'master_agreement_signed', 'pitched_to_licensing_agent', 'notes', 'disco.ac', 'time_tracked', 'status']\n",
    "\n",
    "project_hours = project_hours.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup price column. Changing format from object that is '$1,200.00' to a float. \n",
    "\n",
    "project_hours_to_float_dtype = ['price', 'total_additional_payments', 'expenses', 'total_after_fees']\n",
    "\n",
    "for item in project_hours_to_float_dtype:\n",
    "    project_hours[item] = project_hours[item].astype(str).str.replace('$', '').str.replace(',', '').str.replace('%', '').astype(float)\n",
    "\n",
    "project_hours_to_datetime_dtype = ['fund_date', 'start_date', 'finish_date']\n",
    "\n",
    "for item in project_hours_to_datetime_dtype:\n",
    "    project_hours[item] = project_hours[item].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 'songwriter' column boolean\n",
    "\n",
    "project_hours['songwriter'].fillna('NaN')\n",
    "project_hours['songwriter'] = project_hours['songwriter'].astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate project hourly\n",
    "project_hours['project_hourly'] = project_hours.apply(lambda row: round(row['total_after_fees'] / \n",
    "                                                                        row['total_hours'], 2) if row['total_hours'] > 0 else None, axis=1)\n",
    "\n",
    "# calculate days to finish projects\n",
    "project_hours['days_to_finish'] = (project_hours['finish_date'] - project_hours['start_date']).dt.days\n",
    "\n",
    "# calculate days to start projects\n",
    "project_hours['days_to_start'] = (project_hours['start_date'] - project_hours['fund_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Client ID and Song ID Columns\n",
    "\n",
    "# Create list of unique client_name\n",
    "# Make client names strings\n",
    "project_hours['client_string'] = project_hours['client'].apply(lambda x: str(x))\n",
    "\n",
    "# Create list of unique client names\n",
    "client_name = project_hours['client_string'].unique().tolist()\n",
    "\n",
    "# Sort client_name alphabetically\n",
    "client_name.sort()\n",
    "\n",
    "# List of IDs for client_name\n",
    "id_list = [x+1 for x in range(len(client_name))]\n",
    "\n",
    "# Create dataframe of clients and ids\n",
    "client_df = pd.DataFrame({'client_id':id_list, 'client_string':client_name})\n",
    "\n",
    "# Filter original dataframe\n",
    "song_df = project_hours[['client_string', 'project']]\n",
    "\n",
    "# Inner merge to create new dataframe\n",
    "client_project_df = pd.merge(song_df, client_df, on='client_string', how='inner')\n",
    "\n",
    "# Add song_id by index\n",
    "client_project_df['project_id'] = client_project_df.index + 1\n",
    "\n",
    "# Sort dataframe\n",
    "client_project_database = client_project_df.sort_values(by='client_id', ascending=True, ignore_index=True)\n",
    "\n",
    "# Rename columns\n",
    "client_project_database = client_project_database.rename(columns={'client_string':'client'})\n",
    "\n",
    "# Merge dataframes\n",
    "project_hours_with_ids = pd.merge(project_hours, client_project_df, how='left', on=[\"project\", 'client_string'])\n",
    "\n",
    "# Sort by client_id\n",
    "project_hours_with_ids_sorted = project_hours_with_ids.sort_values(by='client_id', ignore_index=True, ascending=True)\n",
    "\n",
    "# drop client_string column\n",
    "project_hours_with_ids_sorted = project_hours_with_ids_sorted.drop(columns=['client_string'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder Columns\n",
    "\n",
    "cols_to_move = ['client', 'client_id', 'project', 'project_id']\n",
    "remaining_cols = [col for col in project_hours_with_ids_sorted.columns if col not in cols_to_move]\n",
    "new_col_order = cols_to_move + remaining_cols\n",
    "\n",
    "# Reindex dataframe\n",
    "project_hours_with_ids_sorted = project_hours_with_ids_sorted[new_col_order]\n",
    "\n",
    "project_hours_with_ids_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client and project keys for internal reference. \n",
    "\n",
    "client_key = client_df\n",
    "project_key = client_project_df[['project_id', 'project']]\n",
    "\n",
    "# Export keys as .csv files\n",
    "client_key.to_csv('client_key.csv', index=False)\n",
    "project_key.to_csv('project_key.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_hours_with_ids_sorted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average project hourly gropued by client\n",
    "paid_projects_finished = project_hours_with_ids_sorted[project_hours_with_ids_sorted['spec_project'] == False]\n",
    "\n",
    "client_hourly = paid_projects_finished['project_hourly'].groupby(paid_projects_finished['client_id']).mean().to_frame(name='avg_hourly').reset_index()\n",
    "\n",
    "# Determine max hourly\n",
    "max_hourly = client_hourly['avg_hourly'].max()\n",
    "\n",
    "# Calculate percent of max hourly\n",
    "client_hourly['project_hourly_percent_of_max'] = client_hourly['avg_hourly'].apply(lambda x: x/max_hourly)\n",
    "\n",
    "# Get total revenue grouped by client\n",
    "client_revenue = paid_projects_finished['price'].groupby(paid_projects_finished['client_id']).sum().to_frame(name='revenue').reset_index()\n",
    "\n",
    "# Determine max revenue\n",
    "max_revenue = client_revenue['revenue'].max()\n",
    "\n",
    "# Calculate percent of max revenue by client\n",
    "client_revenue['client_revenue_percent_of_max'] = client_revenue['revenue'].apply(lambda x: x/max_revenue)\n",
    "\n",
    "# Merge dataframes\n",
    "client_hourly = client_hourly.merge(right=client_revenue, how='left', on='client_id')\n",
    "\n",
    "# Percentage to weight revenue over hourly\n",
    "weight_revenue = 0.6\n",
    "\n",
    "# Calculate client score\n",
    "client_hourly['score'] = (client_hourly['project_hourly_percent_of_max'] * (1 - weight_revenue) + client_hourly['client_revenue_percent_of_max'] * weight_revenue)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difficult clients: list of client_ids\n",
    "difficult_clients = paid_projects_finished[paid_projects_finished['difficult_client'] == True]['client_id'].unique().tolist()\n",
    "\n",
    "# Add column of difficult clients\n",
    "client_hourly['difficult_client'] = client_hourly['client_id'].apply(lambda x: x in difficult_clients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export .csv\n",
    "client_hourly[['client_id', 'avg_hourly', 'revenue', 'score', 'difficult_client']].to_csv('client_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Tracking Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip whitespace on ends of column names\n",
    "time_tracking.columns = time_tracking.columns.str.strip()\n",
    "\n",
    "# set datetime column\n",
    "time_tracking['datetime'] = time_tracking['datetime'].astype('datetime64[ns]')\n",
    "\n",
    "# Cleanup multi-day sessions\n",
    "\n",
    "# back fill NaN values on sate \"Started\" \n",
    "'''\n",
    "We want to copy all the 'Stopped' values to 'Started' to analyze values on 'Started' times \n",
    "to avoid sessions that straddle days\n",
    "'''\n",
    "\n",
    "time_tracking['hours'].fillna(method='bfill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Back fill notes column as well. They are currently lined up with stopped but since we switched to\n",
    "analyzing on the start times we need to move the rows in the 'notes' column as well. \n",
    "'''\n",
    "\n",
    "# Find indices of non-null values\n",
    "value_indices = time_tracking[time_tracking['notes'].notna()].index\n",
    "\n",
    "for idx in value_indices:\n",
    "    if idx > 0: # start at the second row\n",
    "        time_tracking.at[idx - 1, 'notes'] = time_tracking.at[idx, 'notes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter where state is started and stopped\n",
    "time_tracking_started = time_tracking[time_tracking['state'].str.lower() == 'started']\n",
    "time_tracking_stopped = time_tracking[time_tracking['state'].str.lower() == 'stopped']\n",
    "\n",
    "# Reset indices\n",
    "time_tracking_started = time_tracking_started.reset_index(drop=True)\n",
    "time_tracking_stopped = time_tracking_stopped.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tracking_started.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tracking.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tracking_stopped_to_merge = time_tracking_stopped[['client', 'project']]\n",
    "time_tracking_started_to_merge = time_tracking_started.drop(columns=['client', 'project'])\n",
    "\n",
    "merged_df_start_stop = time_tracking_started_to_merge.merge(time_tracking_stopped_to_merge,\n",
    "                                                            left_index=True,\n",
    "                                                            right_index=True,\n",
    "                                                            how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_start_stop.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select desired columns\n",
    "time_tracking = merged_df_start_stop[['activity','datetime', 'hours', 'client', 'project', 'notes']]\n",
    "\n",
    "time_tracking.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate missing values between client and project\n",
    "missing_value = time_tracking[pd.isnull(time_tracking['client']) & ~pd.isnull(time_tracking['project'])]\n",
    "print(f'There are {len(missing_value)} missing values between client and project columns.')\n",
    "missing_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join time_tracking and client_song_database to encode client and project columns\n",
    "time_tracking_merged = pd.merge(time_tracking, client_project_database, how='left', on=['client', 'project'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tracking_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' MinMix scaling that scales all monetary values across the entire dataframe. I chose to not use sci-kit scalers as they process per column. \n",
    "This is an abnormal case and I am using this method to obfuscate my financial data as a whole.\n",
    "'''\n",
    "columns_to_normalize = ['price', 'total_additional_payments', 'expenses', 'total_after_fees', 'project_hourly']\n",
    "min_max_df = project_hours_with_ids_sorted[columns_to_normalize]\n",
    "\n",
    "max_value = 0\n",
    "min_value = 0 # setting to zero so that $0.00 is the minimum to keep scaling among all columns\n",
    "\n",
    "# iterate through columns in dataframe to max value\n",
    "for column in min_max_df.columns:\n",
    "    max_column_value = min_max_df[column].max(skipna= True)\n",
    "    if max_column_value > max_value:\n",
    "        max_value = max_column_value\n",
    "\n",
    "# define min max function\n",
    "def min_max_global_scaler(x, min, max):\n",
    "    x_scaled = (x - min)/(max - min)\n",
    "    return x_scaled\n",
    "\n",
    "# copy dataframe\n",
    "project_hours_normalized = project_hours_with_ids_sorted.copy()\n",
    "\n",
    "# normalize each column\n",
    "columns_to_normalize = ['price', 'total_additional_payments', 'expenses', 'total_after_fees', 'project_hourly']\n",
    "project_hours_normalized[columns_to_normalize] = project_hours_normalized[columns_to_normalize].apply(min_max_global_scaler, args=(min_value, max_value,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove artist and song_name from project_hours for public viewing\n",
    "project_hours_normalized = project_hours_normalized.drop(columns= ['client', 'project'])\n",
    "\n",
    "# drop client and project columns from time_tracking\n",
    "time_tracking_public = time_tracking_merged.copy().drop(columns= ['client', 'project'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export .csv files for public analysis\n",
    "\n",
    "# Define the target folder where you want to save the CSV files\n",
    "target_folder = os.path.expanduser('~/Dropbox/01 Matt/Tryba Documents/GitHub/Portfolio-Projects')\n",
    "\n",
    "# Define the filenames for your CSV files\n",
    "time_tracking_filename = 'time-tracking.csv'\n",
    "project_hours_normalized_filename = 'project-hours.csv'\n",
    "\n",
    "# Create the full paths for saving the CSV files\n",
    "time_tracking_path = os.path.join(target_folder + '/01 Time Tracking Personal Productivity', time_tracking_filename)\n",
    "project_hours_normalized_path = os.path.join(target_folder + '/02 Music Business Client Analysis', project_hours_normalized_filename)\n",
    "\n",
    "# Save the CSV files\n",
    "time_tracking_public.to_csv(time_tracking_path, index=False)\n",
    "project_hours_normalized.to_csv(project_hours_normalized_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export .csv files for private analysis\n",
    "time_tracking_merged.to_csv('time-tracking.csv', index=False)\n",
    "project_hours_with_ids_sorted.to_csv('project-hours.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tryba-automation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
